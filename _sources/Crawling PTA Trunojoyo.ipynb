{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNhpLEqPc8f3XGwDgun0iGo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Crawling Website PTA Trunojoyo"],"metadata":{"id":"XOInFae6pDbb"}},{"cell_type":"markdown","source":["Crawling (atau web crawling) adalah proses otomatis untuk mengambil informasi dari berbagai halaman web dengan cara mengakses dan mengekstrak data dari berbagai situs secara sistematis. Tujuan dari crawling adalah untuk mengumpulkan informasi yang dapat digunakan untuk berbagai keperluan, seperti analisis data, pemantauan, pengindeksan, dan lain-lain.\n","\n","Crawling dilakukan oleh program komputer yang disebut \"crawler\" atau \"spider\". Crawler ini mengunjungi halaman-halaman web, mengambil konten, mengikuti tautan, dan terus bergerak di seluruh situs web untuk mengumpulkan data. Proses crawling sering digunakan dalam pengembangan mesin pencari seperti Google, analisis data web, riset pasar, dan banyak aplikasi lainnya."],"metadata":{"id":"gTYE08gQYS0E"}},{"cell_type":"markdown","source":["## 1. Install Library BeautifulSoup"],"metadata":{"id":"TLMQMrLcpIvN"}},{"cell_type":"markdown","source":["Library Beautiful Soup adalah salah satu alat yang sering digunakan untuk melakukan crawling dan ekstraksi data dari dokumen HTML dan XML. Beautiful Soup menyediakan cara yang mudah untuk menavigasi dan memanipulasi struktur dokumen HTML. Ini memungkinkan Anda melakukan pencarian elemen berdasarkan tag, atribut, dan teks yang ada di dalam elemen."],"metadata":{"id":"Do1VzSfBYdIe"}},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vUrJstPpntCB","executionInfo":{"status":"ok","timestamp":1692946465989,"user_tz":-420,"elapsed":4570,"user":{"displayName":"20-126 Ahmad Rosyihuddin","userId":"03632124637273514654"}},"outputId":"0be9c9a7-9e1b-49e8-bff7-7e5c5a954f23"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n"]}],"source":["!pip install beautifulsoup4"]},{"cell_type":"markdown","source":["## 2. Import Library"],"metadata":{"id":"WT-DFQLdpNtd"}},{"cell_type":"code","source":["from bs4 import BeautifulSoup\n","import requests\n","import csv"],"metadata":{"id":"Jd6R87A3pRB7","executionInfo":{"status":"ok","timestamp":1692946465991,"user_tz":-420,"elapsed":21,"user":{"displayName":"20-126 Ahmad Rosyihuddin","userId":"03632124637273514654"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## 3. Create Crawling Function"],"metadata":{"id":"gsY5TcSrpSu8"}},{"cell_type":"markdown","source":["### 3.1. Fungsi get_data()"],"metadata":{"id":"yr4-hqKDYw01"}},{"cell_type":"markdown","source":["Fungsi ini mengambil data dari halaman tugas akhir yang diberikan URL-nya. Ia mengekstrak judul tugas akhir, nama penulis, nama dosen pembimbing, dan abstrak dari halaman tersebut."],"metadata":{"id":"_8YiD689ZHcb"}},{"cell_type":"code","source":["def get_data(url):\n","  data_page = []\n","  response = requests.get(url)\n","  if response.status_code == 200:\n","    page = BeautifulSoup(response.content)\n","    container_content = page.find('li', {\"data-id\" : \"id-1\"})\n","    header_page = container_content.find('div', {\"style\" : \"float:left; width:540px;\"})\n","\n","    #  Mengambil judul TA\n","    get_title = header_page.find('a', class_ = 'title').text.strip().split('\\r\\n')\n","    title = \" \".join(get_title)\n","    data_page.append(title)\n","\n","    # Mengambil Penulis dan juga Dospem\n","    creator = header_page.find_all('span')\n","    for i in creator:\n","      splitting_text = i.text.strip().split(':')\n","      if splitting_text[0].strip() == 'Penulis':\n","        data_page.append(splitting_text[1].strip())\n","      elif splitting_text[0].strip() == 'Dosen Pembimbing I':\n","        data_page.append(splitting_text[1].strip())\n","      else:\n","        data_page.append(splitting_text[1].strip())\n","\n","    # Mengambil Abstrak\n","    get_abstrak = container_content.find('p', {\"align\" : \"justify\"}).text.strip().split('\\r\\n')\n","    abstrak = \" \".join(get_abstrak)\n","    data_page.append(abstrak)\n","\n","  return data_page"],"metadata":{"id":"88JBWQjnYpPd","executionInfo":{"status":"ok","timestamp":1692946465992,"user_tz":-420,"elapsed":19,"user":{"displayName":"20-126 Ahmad Rosyihuddin","userId":"03632124637273514654"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### 3.2. Fungsi crawling_pta()"],"metadata":{"id":"rigABZbtY2Pw"}},{"cell_type":"markdown","source":["Fungsi ini melakukan crawling atau pengambilan data dari halaman daftar tugas akhir. Ia mengambil URL halaman, mencari setiap elemen daftar tugas akhir, mengekstrak URL untuk halaman detail setiap tugas akhir, dan menggunakan fungsi `get_data()` untuk mendapatkan informasi lebih lanjut."],"metadata":{"id":"08RFyT4eZPpl"}},{"cell_type":"code","source":["def crawling_pta(url):\n","  hasil_crawling = []\n","  response = requests.get(url)\n","  if response.status_code == 200:\n","    page = BeautifulSoup(response.content)\n","    container_content = page.find('ul', class_= 'items')\n","    list_content = container_content.find_all('li')\n","\n","    for content in list_content:\n","      view_button = content.find('a', class_ = 'gray button').get('href')\n","      hasil_crawling.append(get_data(view_button))\n","\n","    next_page = page.find_all('a', class_='pag_button')\n","    for i in next_page:\n","      if i.text.strip() == '>':\n","        return (hasil_crawling, i.get('href'))"],"metadata":{"id":"KayoLXa_n14_","executionInfo":{"status":"ok","timestamp":1692946466563,"user_tz":-420,"elapsed":589,"user":{"displayName":"20-126 Ahmad Rosyihuddin","userId":"03632124637273514654"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### 3.3. Fungsi main()"],"metadata":{"id":"ooFPPndCY6zk"}},{"cell_type":"markdown","source":["Fungsi utama yang mengatur alur scraping. Ia memulai proses crawling dari URL yang diberikan sebanyak `num_page` halaman. Kemudian, ia menyimpan hasil crawling ke dalam file CSV."],"metadata":{"id":"guoLZt_RZXZo"}},{"cell_type":"code","source":["def main(url, num_page):\n","  content, next_page = crawling_pta(url)\n","\n","  for i in range(num_page-1):\n","    more_content, next_page = crawling_pta(next_page)\n","    content += more_content\n","\n","  with open('crawling_pta.csv', mode='w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(['Judul', 'Penulis', 'Dosen Pembimbing I', 'Dosen Pembimbing II', 'Abstrak'])\n","    writer.writerows(content)\n","\n","  print(f'Data Berhasil Di Simpan Kedalam File crawling_pta.csv Dengan Jumlah : {len(content)} Data')"],"metadata":{"id":"fusHwyJyYtQf","executionInfo":{"status":"ok","timestamp":1692946466568,"user_tz":-420,"elapsed":15,"user":{"displayName":"20-126 Ahmad Rosyihuddin","userId":"03632124637273514654"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### 3.4. Memanggil Fungsi main()"],"metadata":{"id":"wGrFYlv3Y-RS"}},{"cell_type":"markdown","source":["Bagian ini menjalankan fungsi `main` dengan memberikan URL halaman pertama dan jumlah halaman yang ingin diambil."],"metadata":{"id":"eDowzrpzZfFU"}},{"cell_type":"code","source":["main('https://pta.trunojoyo.ac.id/c_search/byprod/10', 1)"],"metadata":{"id":"p8oAqUEnYvLP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hasil crawling (judul, penulis, dosen pembimbing, abstrak) disimpan dalam file CSV dengan nama \"crawling_pta.csv\". Setiap baris mewakili satu tugas akhir."],"metadata":{"id":"fCKIKpBTZlG4"}}]}