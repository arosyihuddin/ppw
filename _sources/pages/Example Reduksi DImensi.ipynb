{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMzNiR49RebEj8APQHnDba3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# (Example) Reduksi Dimensi"],"metadata":{"id":"AZoUhFH6s8H4"}},{"cell_type":"markdown","source":["[Perhitungan Manual Spreadsheet](https://docs.google.com/spreadsheets/d/1M3gfGesTUuCrBbXI4e08W91YK04Fbzyf7ffdyNxsilc/edit?usp=sharing)"],"metadata":{"id":"x98ABCu2s-vz"}},{"cell_type":"markdown","source":["1. **Persiapan Data Teks**: Langkah pertama adalah mempersiapkan data teks yang terdiri dari dua dokumen. Dokumen-dokumen ini akan digunakan dalam analisis.\n","\n","2. **Vektorisasi Teks Menggunakan CountVectorizer**: Dalam langkah ini, kita menggunakan CountVectorizer, alat dari Scikit-Learn, untuk mengubah teks menjadi representasi vektor. CountVectorizer menghitung berapa kali setiap kata muncul dalam dokumen. Kita juga menghilangkan kata-kata umum dalam bahasa Inggris (stop words) dan kata-kata yang muncul terlalu sering (lebih dari 85% dari seluruh dokumen).\n","\n","3. **Penerapan Latent Dirichlet Allocation (LDA)**: LDA adalah algoritma yang digunakan untuk mengidentifikasi topik-topik utama dalam dokumen. Dalam contoh ini, kita ingin mengidentifikasi dua topik. LDA memodelkan dokumen sebagai campuran dari topik-topik ini dan menghasilkan distribusi topik untuk setiap dokumen.\n","\n","4. **Menampilkan Topik dan Term**: Langkah terakhir adalah menampilkan hasil LDA. Kita melihat semua topik yang telah diidentifikasi dalam setiap dokumen bersama dengan term-term yang paling relevan dalam tiap topik. Ini memberi kita wawasan tentang bagaimana dokumen-dokumen tersebut terkait dengan topik-topik yang telah diidentifikasi."],"metadata":{"id":"p4Gswd7NZYzf"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4rXLjK6Hs5mx","executionInfo":{"status":"ok","timestamp":1695366238827,"user_tz":-420,"elapsed":1248,"user":{"displayName":"20-126 Ahmad Rosyihuddin","userId":"03632124637273514654"}},"outputId":"0ff12010-a7fd-4c57-dd16-112ae15d0f37"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dokumen #1: saya makan nasi pecel\n","  Topik #1: Skor Topik - 0.17\n","    Term dalam Topik: makan, pecel, mencari, pergi, ke, pasar\n","  Topik #2: Skor Topik - 0.83\n","    Term dalam Topik: pasar, ke, pergi, mencari, pecel, makan\n","Dokumen #2: saya pergi ke pasar mencari nasi\n","  Topik #1: Skor Topik - 0.89\n","    Term dalam Topik: makan, pecel, mencari, pergi, ke, pasar\n","  Topik #2: Skor Topik - 0.11\n","    Term dalam Topik: pasar, ke, pergi, mencari, pecel, makan\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","import numpy as np\n","\n","# Langkah 1: Persiapkan data teks\n","documents = [\n","    \"saya makan nasi pecel\",\n","    \"saya pergi ke pasar mencari nasi\",\n","]\n","\n","# Langkah 2: Vektorisasi teks menggunakan CountVectorizer\n","vectorizer = CountVectorizer(max_df=0.85, stop_words='english')\n","X = vectorizer.fit_transform(documents)\n","\n","# Langkah 3: Menerapkan LDA\n","num_topics = 2  # Jumlah topik yang ingin diidentifikasi\n","lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n","document_topics = lda.fit_transform(X)\n","\n","# Langkah 4: Menampilkan semua topik untuk setiap dokumen dan term dalam tiap topik\n","feature_names = np.array(vectorizer.get_feature_names_out())\n","for i, doc in enumerate(documents):\n","    print(f\"Dokumen #{i + 1}: {doc}\")\n","    for j, topic_weight in enumerate(document_topics[i]):\n","        topik_terms = lda.components_[j].argsort()  # Mengambil 4 term teratas untuk tiap topik\n","        topik_term_names = feature_names[topik_terms]\n","        print(f\"  Topik #{j + 1}: Skor Topik - {topic_weight:.2f}\")\n","        print(f\"    Term dalam Topik: {', '.join(topik_term_names)}\")"]}]}